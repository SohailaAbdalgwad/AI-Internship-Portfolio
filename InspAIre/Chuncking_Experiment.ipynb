{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install langchain langchain-text-splitters pymupdf sentence-transformers chromadb -q\n",
        "\n",
        "import pymupdf\n",
        "from pathlib import Path\n",
        "from typing import List, Dict\n",
        "import re\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.docstore.document import Document\n",
        "import json\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnMALmOOYigS",
        "outputId": "b346b2d1-785a-4590-9543-0b11ac7078fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.4/21.4 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.17.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1bfNGt-DYf9i",
        "outputId": "1023509e-1420-49d4-980f-f60efd2876a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload your instructional PDF files...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9f823d5a-acf8-4b7c-82fb-cc744a2d8936\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9f823d5a-acf8-4b7c-82fb-cc744a2d8936\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Time Management, LEAP Online.pdf to Time Management, LEAP Online.pdf\n",
            "\n",
            "Uploaded files: ['Time Management, LEAP Online.pdf']\n",
            "\n",
            "==================================================\n",
            "Chunking documents...\n",
            "==================================================\n",
            "Processing: Time Management, LEAP Online.pdf\n",
            "\n",
            "Total chunks created: 123\n",
            "Average chunk length: 592 characters\n",
            "\n",
            "==================================================\n",
            "SAMPLE CHUNKS (first 3):\n",
            "==================================================\n",
            "\n",
            "--- Chunk 1 ---\n",
            "Source: Time Management, LEAP Online.pdf\n",
            "Page: 1\n",
            "Section: \n",
            "Content preview: Time Management \n",
            "LEAP Online \n",
            " \n",
            "University of Bolton 2024 \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "Time Management \n",
            "LEAP Online...\n",
            "\n",
            "--- Chunk 2 ---\n",
            "Source: Time Management, LEAP Online.pdf\n",
            "Page: 2\n",
            "Section: Contents\n",
            "Content preview: Time Management \n",
            "LEAP Online \n",
            " \n",
            "University of Bolton 2024 \n",
            " \n",
            " \n",
            "Contents \n",
            " \n",
            "Introduction ...................................................................................................................\n",
            "\n",
            "--- Chunk 3 ---\n",
            "Source: Time Management, LEAP Online.pdf\n",
            "Page: 2\n",
            "Section: Contents\n",
            "Content preview: Time management in Higher Education .................................................................. 4 \n",
            "What are the benefits of better time management? ................................................\n",
            "Saved 123 chunks to training_modules_chunks2.json\n",
            "\n",
            "Downloading chunks file...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_84bf21b4-fcab-4d9e-a736-b732ec57043b\", \"training_modules_chunks.json\", 74936)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Chunking complete!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class InstructionalPDFChunker:\n",
        "    \"\"\"\n",
        "    Chunker optimized for instructional design materials with semantic awareness\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, chunk_size=800, chunk_overlap=150):\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "\n",
        "        # Text splitter with separators optimized for training content\n",
        "        self.splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap,\n",
        "            separators=[\n",
        "                \"\\n\\n\\n\",  # Section breaks\n",
        "                \"\\n\\n\",    # Paragraph breaks\n",
        "                \"\\n\",      # Line breaks\n",
        "                \". \",      # Sentences\n",
        "                \"! \",\n",
        "                \"? \",\n",
        "                \" \",       # Words\n",
        "                \"\"\n",
        "            ],\n",
        "            length_function=len,\n",
        "        )\n",
        "\n",
        "    def extract_text_from_pdf(self, pdf_path: str) -> List[Dict]:\n",
        "        \"\"\"Extract text from PDF with metadata\"\"\"\n",
        "        doc = pymupdf.open(pdf_path)\n",
        "        pages_data = []\n",
        "\n",
        "        for page_num, page in enumerate(doc, start=1):\n",
        "            text = page.get_text()\n",
        "\n",
        "            # Extract potential headings (text in larger font or bold)\n",
        "            blocks = page.get_text(\"dict\")[\"blocks\"]\n",
        "            headings = []\n",
        "\n",
        "            for block in blocks:\n",
        "                if \"lines\" in block:\n",
        "                    for line in block[\"lines\"]:\n",
        "                        for span in line[\"spans\"]:\n",
        "                            # Detect headings by font size or flags\n",
        "                            if span[\"size\"] > 12 or span[\"flags\"] & 2**4:  # Bold flag\n",
        "                                headings.append(span[\"text\"].strip())\n",
        "\n",
        "            pages_data.append({\n",
        "                \"page_num\": page_num,\n",
        "                \"text\": text,\n",
        "                \"headings\": headings,\n",
        "                \"source\": Path(pdf_path).name\n",
        "            })\n",
        "\n",
        "        doc.close()\n",
        "        return pages_data\n",
        "\n",
        "    def detect_section_boundaries(self, text: str) -> List[str]:\n",
        "        \"\"\"Detect natural section boundaries in instructional content\"\"\"\n",
        "        # Common patterns in training materials\n",
        "        patterns = [\n",
        "            r'\\n\\s*(?:Chapter|Module|Section|Lesson|Unit)\\s+\\d+',\n",
        "            r'\\n\\s*(?:Objective|Learning Outcome|Goal)s?:',\n",
        "            r'\\n\\s*(?:Procedure|Steps|Instructions):',\n",
        "            r'\\n\\s*(?:Example|Exercise|Practice):',\n",
        "            r'\\n\\s*(?:Summary|Conclusion|Review):',\n",
        "            r'\\n\\s*\\d+\\.\\s+[A-Z]',  # Numbered sections with capitalized start\n",
        "        ]\n",
        "\n",
        "        sections = []\n",
        "        last_pos = 0\n",
        "\n",
        "        for pattern in patterns:\n",
        "            for match in re.finditer(pattern, text, re.IGNORECASE):\n",
        "                if match.start() > last_pos:\n",
        "                    sections.append(text[last_pos:match.start()].strip())\n",
        "                    last_pos = match.start()\n",
        "\n",
        "        if last_pos < len(text):\n",
        "            sections.append(text[last_pos:].strip())\n",
        "\n",
        "        return [s for s in sections if len(s) > 100]  # Filter very short sections\n",
        "\n",
        "    def chunk_documents(self, pdf_paths: List[str]) -> List[Document]:\n",
        "        \"\"\"Main chunking method for multiple PDFs\"\"\"\n",
        "        all_chunks = []\n",
        "\n",
        "        for pdf_path in pdf_paths:\n",
        "            print(f\"Processing: {pdf_path}\")\n",
        "            pages_data = self.extract_text_from_pdf(pdf_path)\n",
        "\n",
        "            for page_data in pages_data:\n",
        "                text = page_data[\"text\"]\n",
        "\n",
        "                if not text.strip():\n",
        "                    continue\n",
        "\n",
        "                # Try semantic sectioning first\n",
        "                sections = self.detect_section_boundaries(text)\n",
        "\n",
        "                if len(sections) > 1:\n",
        "                    # Process each section separately\n",
        "                    for section in sections:\n",
        "                        chunks = self.splitter.split_text(section)\n",
        "\n",
        "                        for i, chunk in enumerate(chunks):\n",
        "                            # Extract first heading as context\n",
        "                            heading = page_data[\"headings\"][0] if page_data[\"headings\"] else \"No Title\"\n",
        "\n",
        "                            doc = Document(\n",
        "                                page_content=chunk,\n",
        "                                metadata={\n",
        "                                    \"source\": page_data[\"source\"],\n",
        "                                    \"page\": page_data[\"page_num\"],\n",
        "                                    \"chunk_id\": f\"{page_data['source']}_p{page_data['page_num']}_c{i}\",\n",
        "                                    \"section_heading\": heading,\n",
        "                                    \"total_chunks_in_page\": len(chunks)\n",
        "                                }\n",
        "                            )\n",
        "                            all_chunks.append(doc)\n",
        "                else:\n",
        "                    # Fallback to standard chunking\n",
        "                    chunks = self.splitter.split_text(text)\n",
        "\n",
        "                    for i, chunk in enumerate(chunks):\n",
        "                        heading = page_data[\"headings\"][0] if page_data[\"headings\"] else \"No Title\"\n",
        "\n",
        "                        doc = Document(\n",
        "                            page_content=chunk,\n",
        "                            metadata={\n",
        "                                \"source\": page_data[\"source\"],\n",
        "                                \"page\": page_data[\"page_num\"],\n",
        "                                \"chunk_id\": f\"{page_data['source']}_p{page_data['page_num']}_c{i}\",\n",
        "                                \"section_heading\": heading,\n",
        "                                \"total_chunks_in_page\": len(chunks)\n",
        "                            }\n",
        "                        )\n",
        "                        all_chunks.append(doc)\n",
        "\n",
        "        return all_chunks\n",
        "\n",
        "    def save_chunks(self, chunks: List[Document], output_path: str = \"chunks_output.json\"):\n",
        "        \"\"\"Save chunks to JSON for inspection\"\"\"\n",
        "        chunks_data = [\n",
        "            {\n",
        "                \"content\": chunk.page_content,\n",
        "                \"metadata\": chunk.metadata\n",
        "            }\n",
        "            for chunk in chunks\n",
        "        ]\n",
        "\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(chunks_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"Saved {len(chunks)} chunks to {output_path}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Upload your PDFs first (use Colab's file upload)\n",
        "    from google.colab import files\n",
        "\n",
        "    print(\"Upload your instructional PDF files...\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    pdf_paths = list(uploaded.keys())\n",
        "    print(f\"\\nUploaded files: {pdf_paths}\")\n",
        "\n",
        "    # Initialize chunker\n",
        "    chunker = InstructionalPDFChunker(\n",
        "        chunk_size=800,      # Adjust based on your needs\n",
        "        chunk_overlap=150     # Maintain context between chunks\n",
        "    )\n",
        "\n",
        "    # Process PDFs\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Chunking documents...\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    chunks = chunker.chunk_documents(pdf_paths)\n",
        "\n",
        "    # Display statistics\n",
        "    print(f\"\\nTotal chunks created: {len(chunks)}\")\n",
        "    print(f\"Average chunk length: {sum(len(c.page_content) for c in chunks) / len(chunks):.0f} characters\")\n",
        "\n",
        "    # Show sample chunks\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"SAMPLE CHUNKS (first 3):\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    for i, chunk in enumerate(chunks[:3], 1):\n",
        "        print(f\"\\n--- Chunk {i} ---\")\n",
        "        print(f\"Source: {chunk.metadata['source']}\")\n",
        "        print(f\"Page: {chunk.metadata['page']}\")\n",
        "        print(f\"Section: {chunk.metadata['section_heading']}\")\n",
        "        print(f\"Content preview: {chunk.page_content[:200]}...\")\n",
        "\n",
        "    # Save to file\n",
        "    chunker.save_chunks(chunks, \"training_modules_chunks2.json\")\n",
        "\n",
        "    # Download the results\n",
        "    print(\"\\nDownloading chunks file...\")\n",
        "    files.download(\"training_modules_chunks.json\")\n",
        "\n",
        "    print(\"\\n✅ Chunking complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pymupdf\n",
        "from pathlib import Path\n",
        "from typing import List, Dict\n",
        "import re\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.docstore.document import Document\n",
        "import json\n",
        "\n",
        "class InstructionalPDFChunker:\n",
        "    \"\"\"\n",
        "    Chunker optimized for instructional design materials with semantic awareness\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, chunk_size=800, chunk_overlap=150):\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "\n",
        "        # Text splitter with separators optimized for training content\n",
        "        self.splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap,\n",
        "            separators=[\n",
        "                \"\\n\\n\\n\",\n",
        "                \"\\n\\n\",\n",
        "                \"\\n\",\n",
        "                \". \",\n",
        "                \"! \",\n",
        "                \"? \",\n",
        "                \" \",\n",
        "                \"\"\n",
        "            ],\n",
        "            length_function=len,\n",
        "        )\n",
        "\n",
        "    def extract_text_from_pdf(self, pdf_path: str) -> List[Dict]:\n",
        "        \"\"\"Extract text from PDF with metadata\"\"\"\n",
        "        doc = pymupdf.open(pdf_path)\n",
        "        pages_data = []\n",
        "\n",
        "        for page_num, page in enumerate(doc, start=1):\n",
        "            text = page.get_text()\n",
        "\n",
        "            # Extract potential headings (text in larger font or bold)\n",
        "            blocks = page.get_text(\"dict\")[\"blocks\"]\n",
        "            headings = []\n",
        "\n",
        "            for block in blocks:\n",
        "                if \"lines\" in block:\n",
        "                    for line in block[\"lines\"]:\n",
        "                        for span in line[\"spans\"]:\n",
        "                            # Detect headings by font size or flags\n",
        "                            if span[\"size\"] > 12 or span[\"flags\"] & 2**4:\n",
        "                                headings.append(span[\"text\"].strip())\n",
        "\n",
        "            pages_data.append({\n",
        "                \"page_num\": page_num,\n",
        "                \"text\": text,\n",
        "                \"headings\": headings,\n",
        "                \"source\": Path(pdf_path).name\n",
        "            })\n",
        "\n",
        "        doc.close()\n",
        "        return pages_data\n",
        "\n",
        "    def is_irrelevant_section(self, text: str) -> bool:\n",
        "        \"\"\"Detect if text is from irrelevant sections\"\"\"\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        # Patterns for sections to ignore\n",
        "        irrelevant_patterns = [\n",
        "            'references',\n",
        "            'bibliography',\n",
        "            'works cited',\n",
        "            'about the author',\n",
        "            'author information',\n",
        "            'author bio',\n",
        "            'acknowledgment',\n",
        "            'table of contents',\n",
        "            'index',\n",
        "            'appendix',\n",
        "            'copyright',\n",
        "            'isbn',\n",
        "            'published by'\n",
        "        ]\n",
        "\n",
        "        # Check if the text starts with these headings\n",
        "        text_start = text_lower[:100]\n",
        "        for pattern in irrelevant_patterns:\n",
        "            if pattern in text_start:\n",
        "                return True\n",
        "\n",
        "        # Check for citation-heavy text (likely references section)\n",
        "        citation_patterns = [\n",
        "            r'\\([12]\\d{3}\\)',\n",
        "            r'\\bet\\s+al\\.',\n",
        "            r'\\bpp?\\.\\s*\\d+',\n",
        "            r'doi:',\n",
        "            r'http[s]?://(?:dx\\.)?doi\\.org',\n",
        "        ]\n",
        "\n",
        "        citation_count = sum(len(re.findall(pattern, text_lower)) for pattern in citation_patterns)\n",
        "        words = len(text.split())\n",
        "\n",
        "        # If more than 15% are citations, likely a reference section\n",
        "        if words > 20 and citation_count > words * 0.15:\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        \"\"\"Clean text by removing headers, footers, and page numbers\"\"\"\n",
        "        lines = text.split('\\n')\n",
        "        cleaned_lines = []\n",
        "\n",
        "        for line in lines:\n",
        "            line_stripped = line.strip()\n",
        "\n",
        "            # Skip very short lines\n",
        "            if len(line_stripped) < 5:\n",
        "                continue\n",
        "\n",
        "            # Skip page numbers (standalone numbers)\n",
        "            if re.match(r'^\\d+$', line_stripped):\n",
        "                continue\n",
        "\n",
        "            # Skip copyright notices\n",
        "            if re.match(r'^©|^copyright', line_stripped, re.IGNORECASE):\n",
        "                continue\n",
        "\n",
        "            cleaned_lines.append(line)\n",
        "\n",
        "        return '\\n'.join(cleaned_lines)\n",
        "\n",
        "    def detect_section_boundaries(self, text: str) -> List[str]:\n",
        "        \"\"\"Detect natural section boundaries in instructional content\"\"\"\n",
        "        patterns = [\n",
        "            r'\\n\\s*(?:Chapter|Module|Section|Lesson|Unit)\\s+\\d+',\n",
        "            r'\\n\\s*(?:Objective|Learning Outcome|Goal)s?:',\n",
        "            r'\\n\\s*(?:Procedure|Steps|Instructions):',\n",
        "            r'\\n\\s*(?:Example|Exercise|Practice):',\n",
        "            r'\\n\\s*(?:Summary|Conclusion|Review):',\n",
        "            r'\\n\\s*\\d+\\.\\s+[A-Z]',\n",
        "        ]\n",
        "\n",
        "        sections = []\n",
        "        last_pos = 0\n",
        "\n",
        "        for pattern in patterns:\n",
        "            for match in re.finditer(pattern, text, re.IGNORECASE):\n",
        "                if match.start() > last_pos:\n",
        "                    sections.append(text[last_pos:match.start()].strip())\n",
        "                    last_pos = match.start()\n",
        "\n",
        "        if last_pos < len(text):\n",
        "            sections.append(text[last_pos:].strip())\n",
        "\n",
        "        return [s for s in sections if len(s) > 100]\n",
        "\n",
        "    def chunk_documents(self, pdf_paths: List[str]) -> List[Document]:\n",
        "        \"\"\"Main chunking method for multiple PDFs\"\"\"\n",
        "        all_chunks = []\n",
        "\n",
        "        for pdf_path in pdf_paths:\n",
        "            print(f\"Processing: {pdf_path}\")\n",
        "            pages_data = self.extract_text_from_pdf(pdf_path)\n",
        "\n",
        "            for page_data in pages_data:\n",
        "                text = page_data[\"text\"]\n",
        "\n",
        "                if not text.strip():\n",
        "                    continue\n",
        "\n",
        "                # Clean the text\n",
        "                text = self.clean_text(text)\n",
        "\n",
        "                # Skip if this is an irrelevant section\n",
        "                if self.is_irrelevant_section(text):\n",
        "                    print(f\"  Skipping irrelevant section on page {page_data['page_num']}\")\n",
        "                    continue\n",
        "\n",
        "                # Try semantic sectioning first\n",
        "                sections = self.detect_section_boundaries(text)\n",
        "\n",
        "                if len(sections) > 1:\n",
        "                    for section in sections:\n",
        "                        if self.is_irrelevant_section(section):\n",
        "                            continue\n",
        "\n",
        "                        chunks = self.splitter.split_text(section)\n",
        "\n",
        "                        for i, chunk in enumerate(chunks):\n",
        "                            heading = page_data[\"headings\"][0] if page_data[\"headings\"] else \"No Title\"\n",
        "\n",
        "                            doc = Document(\n",
        "                                page_content=chunk,\n",
        "                                metadata={\n",
        "                                    \"source\": page_data[\"source\"],\n",
        "                                    \"page\": page_data[\"page_num\"],\n",
        "                                    \"chunk_id\": f\"{page_data['source']}_p{page_data['page_num']}_c{i}\",\n",
        "                                    \"section_heading\": heading,\n",
        "                                    \"total_chunks_in_page\": len(chunks)\n",
        "                                }\n",
        "                            )\n",
        "                            all_chunks.append(doc)\n",
        "                else:\n",
        "                    chunks = self.splitter.split_text(text)\n",
        "\n",
        "                    for i, chunk in enumerate(chunks):\n",
        "                        heading = page_data[\"headings\"][0] if page_data[\"headings\"] else \"No Title\"\n",
        "\n",
        "                        doc = Document(\n",
        "                            page_content=chunk,\n",
        "                            metadata={\n",
        "                                \"source\": page_data[\"source\"],\n",
        "                                \"page\": page_data[\"page_num\"],\n",
        "                                \"chunk_id\": f\"{page_data['source']}_p{page_data['page_num']}_c{i}\",\n",
        "                                \"section_heading\": heading,\n",
        "                                \"total_chunks_in_page\": len(chunks)\n",
        "                            }\n",
        "                        )\n",
        "                        all_chunks.append(doc)\n",
        "\n",
        "        return all_chunks\n",
        "\n",
        "    def save_chunks(self, chunks: List[Document], output_path: str = \"chunks_output.json\"):\n",
        "        \"\"\"Save chunks to JSON for inspection\"\"\"\n",
        "        chunks_data = [\n",
        "            {\n",
        "                \"content\": chunk.page_content,\n",
        "                \"metadata\": chunk.metadata\n",
        "            }\n",
        "            for chunk in chunks\n",
        "        ]\n",
        "\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(chunks_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"Saved {len(chunks)} chunks to {output_path}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    from google.colab import files\n",
        "\n",
        "    print(\"Upload your instructional PDF files...\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    pdf_paths = list(uploaded.keys())\n",
        "    print(f\"\\nUploaded files: {pdf_paths}\")\n",
        "\n",
        "    # Initialize chunker\n",
        "    chunker = InstructionalPDFChunker(\n",
        "        chunk_size=800,\n",
        "        chunk_overlap=150\n",
        "    )\n",
        "\n",
        "    # Process PDFs\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Chunking documents...\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    chunks = chunker.chunk_documents(pdf_paths)\n",
        "\n",
        "    # Display statistics\n",
        "    print(f\"\\nTotal chunks created: {len(chunks)}\")\n",
        "    print(f\"Average chunk length: {sum(len(c.page_content) for c in chunks) / len(chunks):.0f} characters\")\n",
        "\n",
        "    # Show sample chunks\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"SAMPLE CHUNKS (first 3):\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    for i, chunk in enumerate(chunks[:3], 1):\n",
        "        print(f\"\\n--- Chunk {i} ---\")\n",
        "        print(f\"Source: {chunk.metadata['source']}\")\n",
        "        print(f\"Page: {chunk.metadata['page']}\")\n",
        "        print(f\"Section: {chunk.metadata['section_heading']}\")\n",
        "        print(f\"Content preview: {chunk.page_content[:200]}...\")\n",
        "\n",
        "    # Save to file\n",
        "    chunker.save_chunks(chunks, \"training_modules_chunks.json\")\n",
        "\n",
        "    # Download the results\n",
        "    print(\"\\nDownloading chunks file...\")\n",
        "    files.download(\"training_modules_chunks.json\")\n",
        "\n",
        "    print(\"\\n✅ Chunking complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 871
        },
        "id": "6a5blfLAYzqL",
        "outputId": "d9e62a1c-c22d-415c-e83f-d7a2ebb10621"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload your instructional PDF files...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3d1fc844-9f96-4abc-9947-506cd121dd59\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3d1fc844-9f96-4abc-9947-506cd121dd59\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Time Switching The Advanced Method of Time Management for Self-Education (Sazonov, Victor) (Z-Library).pdf to Time Switching The Advanced Method of Time Management for Self-Education (Sazonov, Victor) (Z-Library).pdf\n",
            "\n",
            "Uploaded files: ['Time Switching The Advanced Method of Time Management for Self-Education (Sazonov, Victor) (Z-Library).pdf']\n",
            "\n",
            "==================================================\n",
            "Chunking documents...\n",
            "==================================================\n",
            "Processing: Time Switching The Advanced Method of Time Management for Self-Education (Sazonov, Victor) (Z-Library).pdf\n",
            "  Skipping irrelevant section on page 7\n",
            "  Skipping irrelevant section on page 33\n",
            "  Skipping irrelevant section on page 34\n",
            "\n",
            "Total chunks created: 56\n",
            "Average chunk length: 589 characters\n",
            "\n",
            "==================================================\n",
            "SAMPLE CHUNKS (first 3):\n",
            "==================================================\n",
            "\n",
            "--- Chunk 1 ---\n",
            "Source: Time Switching The Advanced Method of Time Management for Self-Education (Sazonov, Victor) (Z-Library).pdf\n",
            "Page: 2\n",
            "Section: \n",
            "Content preview: TIME SWITCHING\n",
            "The Advanced Method of Time Management\n",
            "for Self-Education\n",
            "Victor Sazonov...\n",
            "\n",
            "--- Chunk 2 ---\n",
            "Source: Time Switching The Advanced Method of Time Management for Self-Education (Sazonov, Victor) (Z-Library).pdf\n",
            "Page: 5\n",
            "Section: All rights reserved. No part of this publication may be reproduced,\n",
            "Content preview: All rights reserved. No part of this publication may be reproduced,\n",
            "distributed, or transmitted in any form or by any means, including\n",
            "photocopying, recording, or other electronic or mechanical method...\n",
            "\n",
            "--- Chunk 3 ---\n",
            "Source: Time Switching The Advanced Method of Time Management for Self-Education (Sazonov, Victor) (Z-Library).pdf\n",
            "Page: 6\n",
            "Section: To my mom and dad\n",
            "Content preview: To my mom and dad...\n",
            "Saved 56 chunks to training_modules_chunks.json\n",
            "\n",
            "Downloading chunks file...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0f799941-16ff-4108-99a9-48ca95d7754b\", \"training_modules_chunks.json\", 58020)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Chunking complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jcw5VYOhaTcp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}